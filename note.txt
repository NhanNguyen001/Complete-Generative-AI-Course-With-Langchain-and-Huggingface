1. What is tokenization?
tokenization is a process to convert either a paragraph or a sentences into token.